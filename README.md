# PDF-Insight-Retrieval-System
.

ğŸš€ Local RAG System using LangChain, Hugging Face & ChromaDB

This project implements a local Retrieval-Augmented Generation (RAG) pipeline using LangChain, Hugging Face embeddings, and a lightweight Phi-3 Mini LLM.
It allows users to upload or load a PDF, retrieve the most relevant document chunks, and generate context-aware answers using a locally running model.

ğŸ“Œ Features

ğŸ” PDF loading & preprocessing using PyPDFLoader and RecursiveCharacterTextSplitter.

ğŸ§  Semantic embeddings generated with sentence-transformers/all-mpnet-base-v2.

ğŸ—‚ï¸ ChromaDB local vector store with persistent storage.

ğŸ¤– Local LLM (Phi-3-mini-4k-instruct) integrated using HuggingFacePipeline for fast, offline generation.

ğŸ§¾ Context-aware responses using LangChain's prompt templates and chain execution.

ğŸ” Automatically detects and loads existing ChromaDB or creates a new one.

ğŸ› ï¸ Tech Stack

Python

LangChain

ChromaDB

Hugging Face Transformers

Sentence Transformers

âš™ï¸ Installation
1ï¸âƒ£ Clone the repository
2ï¸âƒ£ Create and activate virtual environment
3ï¸âƒ£ Install dependencies
â–¶ï¸ Usage
Run the RAG system:

Type your question related to the content inside ML.pdf.

The system will:

Load the PDF

Split into semantic chunks

Retrieve top-k relevant chunks

Pass context + question to your local LLM

Output a context-grounded answer
ğŸ“ˆ Future Improvements

Add Streamlit UI for interactive chat

Support for multiple document uploads

Replace Phi-3 Mini with larger models (e.g., Llama-3 8B)

Add caching for faster retrieval

Add evaluation metrics (RAGAS, embeddings similarity score)
Phi-3 Mini LLM

LangChain Community Libraries
